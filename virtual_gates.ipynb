{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Virtual Gate Crossing\n",
    "\n",
    "This notebook demonstrations a parallel, distributed, share-nothing spatial join between a relatively large dataset and a small dataset.\n",
    "\n",
    "In this case, virtual gates are defined at various locations in a port, and the outcome is an account of the number of crossings of these gates by ships using their AIS target positions.\n",
    "\n",
    "Note that the join is to a \"small\" spatial dataset that we can:\n",
    "\n",
    "- [Broadcast](https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables) to all the spark workers.\n",
    "- Brutly traverse it on each worker, as it is cheaper and faster to do so that spatially index it.\n",
    "\n",
    "To get started, make sure to install [shapely](https://shapely.readthedocs.io/en/latest/) in the current active conda environment using the command line:\n",
    "\n",
    "```\n",
    "conda install shapely\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import arcgis\n",
    "from spark_esri import spark_start, spark_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop existing spark instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a spark instance with user defined configurtations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"spark.driver.memory\":\"16G\"\n",
    "}\n",
    "spark = spark_start(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined a global spatial reference to be used throught the notebook.\n",
    "\n",
    "All coordinates will be in web mercator meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_ref = arcpy.SpatialReference(3857)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into a broadcast variable the virtual gates.\n",
    "\n",
    "A gate is a feature has:\n",
    "\n",
    "- A LineString shape type.\n",
    "- One attribute named `GateID` of type `Long`.\n",
    "\n",
    "Note that the shape is read in [WKB](https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry) format to enable its serialization during the broadcast internal mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"GateID\",\"SHAPE@WKB\"]\n",
    "with arcpy.da.SearchCursor(\"Gates\", fields, spatial_reference=sp_ref) as rows:\n",
    "    bv = spark.sparkContext.broadcast(list(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the AIS target locations.\n",
    "\n",
    "Note that the target timestamp is converted to an epoch value (seconds since 1970)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"SHAPE@X\",\"SHAPE@Y\",\"MMSI\",\"BaseDateTime\"]\n",
    "with arcpy.da.SearchCursor(\"Broadcast\", fields, spatial_reference=sp_ref) as rows:\n",
    "    spark\\\n",
    "        .createDataFrame(rows, \"x double,y double,mmsi string,t timestamp\")\\\n",
    "        .selectExpr(\"mmsi\",\"x\",\"y\",\"unix_timestamp(t) t\")\\\n",
    "        .createOrReplaceTempView(\"v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A micropath is created from the current record and the leading record based on the `mmsi` field and the epoch `t` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark\\\n",
    "    .sql(\"\"\"\n",
    "select mmsi,\n",
    "x x1,\n",
    "y y1,\n",
    "t t1,\n",
    "lead(x,1,0.0) over (partition by mmsi order by t) x2,\n",
    "lead(y,1,0.0) over (partition by mmsi order by t) y2,\n",
    "lead(t,1,0) over (partition by mmsi order by t) t2\n",
    "from v0\n",
    "\"\"\")\\\n",
    "    .createOrReplaceTempView(\"v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each micropath is enriched with the vertical and horizontal displacement and the time difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select *,(x2-x1) dx,(y2-y1) dy,(t2-t1) dt from v1 where t1 < t2\").createOrReplaceTempView(\"v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich with travel distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select mmsi,x1,y1,x2,y2,sqrt(dx*dx+dy*dy) dd,dt from v2\").createOrReplaceTempView(\"v3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enrich with travel velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select *,dd/dt mps from v3\").createOrReplaceTempView(\"v4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove noisy micropaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.sql(\"\"\"\n",
    "select x1,y1,x2,y2\n",
    "from v4\n",
    "where dd between 1 and 1500\n",
    "and mps < 25\n",
    "and dt < 130\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defined a function to be execute on each micropath partition.\n",
    "\n",
    "The function accepts a set of micropaths as an argument and will:\n",
    "\n",
    "- Read the broadcasted gates once at start of the function and will convert the gates to shapely geometries.\n",
    "- Iterate through every micropath and will check if it intersecs with of a gate.\n",
    "- If an intersection is detected, then the intersection point is emitted and so is the direction of travel.\n",
    "- The direction of travel is defined as `LR` (left to right) or `RL` (right to left).  The following illustrates what is defined as left and right. This is relative to the how gate is \"drawn\" on the map.\n",
    "\n",
    "![](media/Gates0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(partition):\n",
    "    from shapely.wkb import loads\n",
    "    from shapely.geometry import LineString\n",
    "    \n",
    "    def name_geom(_g):\n",
    "        \"\"\"Function to convert WKB to a shapely geometry and line vector.\n",
    "        \"\"\"\n",
    "        geom = loads(bytes(_g[1]))\n",
    "        coords = geom.coords\n",
    "        head = coords[0]\n",
    "        last = coords[-1]\n",
    "        vx = last[0] - head[0]\n",
    "        vy = last[1] - head[1]\n",
    "        return _g[0],geom,vx,vy\n",
    "        \n",
    "    # Read the gates in WKB format from the broadcast variable.\n",
    "    gates = [name_geom(v) for v in bv.value]\n",
    "        \n",
    "    # Perform cartesian product of paths and gates.\n",
    "    for micropath in partition:\n",
    "        x1 = micropath[\"x1\"]\n",
    "        y1 = micropath[\"y1\"]\n",
    "        x2 = micropath[\"x2\"]\n",
    "        y2 = micropath[\"y2\"]\n",
    "        px = x2 - x1\n",
    "        py = y2 - y1\n",
    "        path = LineString([(x1,y1),(x2,y2)])\n",
    "        for gate_id, gate_geom, gx, gy in gates:\n",
    "            point = gate_geom.intersection(path)\n",
    "            if not point.is_empty:\n",
    "                # Calculate the cross product between the micropath vector and the gate vector.\n",
    "                cross = px * gy - py * gx\n",
    "                lr_rl = \"RL\" if cross < 0.0 else \"LR\" \n",
    "                yield point.x,point.y,gate_id,lr_rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the distributed spatial join.\n",
    "\n",
    "As of this writing and in Spark 2.X in python, a dataframe has to be converted to an RDD to apply a function on each distributed partition and then converted back to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1\\\n",
    "    .rdd\\\n",
    "    .mapPartitions(func)\\\n",
    "    .toDF([\"gate_x\",\"gate_y\",\"gate_id\",\"travel_dir\"])\\\n",
    "    .cache()\n",
    "\n",
    "df2.createOrReplaceTempView(\"v5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect on the driver all the intersection points at each gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = df2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a point feature class of the collected points to be placed in the TOC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = \"memory\"\n",
    "nm = \"GatePoints\"\n",
    "\n",
    "fc = os.path.join(ws,nm)\n",
    "\n",
    "arcpy.management.Delete(fc)\n",
    "\n",
    "arcpy.management.CreateFeatureclass(ws, nm, \"POINT\", spatial_reference=sp_ref)\n",
    "arcpy.management.AddField(fc, \"GATE_ID\", \"LONG\")\n",
    "arcpy.management.AddField(fc, \"TRAVEL_DIR\", \"TEXT\")\n",
    "\n",
    "with arcpy.da.InsertCursor(fc, [\"SHAPE@X\",\"SHAPE@Y\",\"GATE_ID\",\"TRAVEL_DIR\"]) as cursor:\n",
    "    for row in rows:\n",
    "        cursor.insertRow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the statistics for each gate and its travel direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = spark.sql(\"\"\"\n",
    "select gate_id,travel_dir,count(1) cnt\n",
    "from v5\n",
    "group by gate_id,travel_dir\n",
    "order by gate_id,travel_dir\n",
    "\"\"\")\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table of the statistics and place it in the TOC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = \"memory\"\n",
    "nm = \"GateStats\"\n",
    "\n",
    "fc = os.path.join(ws,nm)\n",
    "\n",
    "arcpy.management.Delete(fc)\n",
    "\n",
    "arcpy.management.CreateTable(ws, nm)\n",
    "arcpy.management.AddField(fc, \"GATE_ID\", \"LONG\")\n",
    "arcpy.management.AddField(fc, \"TRAVEL_DIR\", \"TEXT\")\n",
    "arcpy.management.AddField(fc, \"TRAVEL_CNT\", \"LONG\")\n",
    "\n",
    "with arcpy.da.InsertCursor(fc, [\"GATE_ID\",\"TRAVEL_DIR\",\"TRAVEL_CNT\"]) as cursor:\n",
    "    for row in rows:\n",
    "        cursor.insertRow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the spark instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
